{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and a Simple Baseline\n",
    "\n",
    "In this part of the assignment, we'll introduce the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) (SST) dataset, and train a Naive Bayes model as a simple baseline. The SST, introduced by [(Socher et al. 2013)](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) consists of approximately 10,000 sentences from movie reviews associated with fine-grained sentiment labels on a five-point scale, and is a popular benchmark for text classification.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Part (a):** The Stanford Sentiment Treebank\n",
    "- **Part (b):** Naive Bayes\n",
    "- **Part (c):** Exploring Negation\n",
    "\n",
    "Exercises are interspersed throughout the notebook. Be sure you catch all of them! There are 4 questions for Part (a), 2 for Part (b), and 3 for Part (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 10.8MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.0.3\n",
      "    Uninstalling pip-19.0.3:\n",
      "      Successfully uninstalled pip-19.0.3\n",
      "Successfully installed pip-19.1.1\n",
      "Uninstalling tensorflow-1.12.0:\n",
      "  Successfully uninstalled tensorflow-1.12.0\n",
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/8c/7608ba709bd536bc2bccb0d1abbb70aafe9cf7e0170353b4b720ed54cb71/tensorflow-1.14.0-cp36-cp36m-macosx_10_11_x86_64.whl (105.8MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105.8MB 267kB/s  eta 0:00:01    |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 23.5MB 12.7MB/s eta 0:00:07     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 47.7MB 14.4MB/s eta 0:00:05     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 62.7MB 23.4MB/s eta 0:00:02     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 95.0MB 30.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel>=0.26 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/10/44230dd6bf3563b8f227dbf344c908d412ad2ff48066476672f3a72e174e/wheel-0.33.4-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d9/f7a3e3fb44de98c85991fca727238c9ab8ca264e9edb2dbfd00849395d6d/protobuf-3.8.0-cp36-cp36m-macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (1.4MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 11.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/61/1bf3bc7ed4772d81b480e6934c526707c7b799a359fea5c32d61b9715b48/grpcio-1.21.1-cp36-cp36m-macosx_10_9_x86_64.whl (2.0MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0MB 13.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.10.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 3.9MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 10.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/c9/3526a357b6c35e5529158fbcfac1bb3adc8827e8809a6d254019d326d1cc/numpy-1.16.4-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (13.9MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.9MB 9.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 5.0MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 491kB 17.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 14.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 16.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools (from protobuf>=3.6.1->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl (575kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 583kB 14.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py (from keras-applications>=1.0.6->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/21/1cdf7fa7868528b35c1a08a770eb9334279574a8b5f1d7a2966dcec14e42/h5py-2.9.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (6.3MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.3MB 11.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/57/92a497e38161ce40606c27a86759c6b92dd34fcdb33f64171ec559257c02/Werkzeug-0.15.4-py2.py3-none-any.whl (327kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 327kB 15.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 11.1MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, gast, wrapt, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/anishphilljoe/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/anishphilljoe/Library/Caches/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/anishphilljoe/Library/Caches/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/anishphilljoe/Library/Caches/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "Successfully built termcolor gast wrapt absl-py\n",
      "\u001b[31mERROR: distributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: wheel, six, setuptools, protobuf, termcolor, grpcio, astor, numpy, h5py, keras-applications, keras-preprocessing, gast, wrapt, google-pasta, tensorflow-estimator, absl-py, werkzeug, markdown, tensorboard, tensorflow\n",
      "  Found existing installation: wheel 0.31.1\n",
      "    Uninstalling wheel-0.31.1:\n",
      "      Successfully uninstalled wheel-0.31.1\n",
      "  Found existing installation: six 1.11.0\n",
      "    Uninstalling six-1.11.0:\n",
      "      Successfully uninstalled six-1.11.0\n",
      "  Found existing installation: setuptools 39.1.0\n",
      "    Uninstalling setuptools-39.1.0:\n",
      "      Successfully uninstalled setuptools-39.1.0\n",
      "  Found existing installation: protobuf 3.6.1\n",
      "    Uninstalling protobuf-3.6.1:\n",
      "      Successfully uninstalled protobuf-3.6.1\n",
      "  Found existing installation: termcolor 1.1.0\n",
      "\u001b[31mERROR: Cannot uninstall 'termcolor'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Fix TensorFlow installation after A2.\n",
    "!pip install --upgrade pip\n",
    "!pip uninstall -y tensorflow\n",
    "!pip install --force-reinstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a few python packages using pip\n",
    "from w266_common import utils\n",
    "utils.require_package(\"wget\")      # for fetching dataset\n",
    "utils.require_package(\"bokeh\")     # for plotting histograms\n",
    "utils.require_package(\"graphviz\")  # for rendering trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries: GraphViz\n",
    "\n",
    "This notebook uses [GraphViz](https://www.graphviz.org/) to render tree structures. On Ubuntu / Debian (including Google Cloud), you can install it by running on the command line:\n",
    "```\n",
    "sudo apt-get install graphviz\n",
    "```\n",
    "\n",
    "For Mac OSX, you can install using Homebrew:\n",
    "```\n",
    "brew install graphviz\n",
    "```\n",
    "or see https://www.graphviz.org/download/ for more options. Run the cell below to set up rendering and show a sample tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\n",
      "Sample tree to test rendering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4>I love W266 ðŸ˜„</h4>\n",
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: tree Pages: 1 -->\n",
       "<svg width=\"278pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 278.00 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<title>tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-256 274,-256 274,4 -4,4\"/>\n",
       "<!-- 3 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>3</title>\n",
       "<g id=\"a_node1\"><a xlink:title=\"I\">\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"54,-36 0,-36 0,0 54,0 54,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">I</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<g id=\"a_node2\"><a xlink:title=\"love\">\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"126,-36 72,-36 72,0 126,0 126,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">love</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#cccccc\" d=\"M54.2813,-18C60.013,-18 65.7448,-18 71.4766,-18\"/>\n",
       "<polygon fill=\"#cccccc\" stroke=\"#cccccc\" points=\"71.7495,-18 71.7495,-18 71.7495,-18 71.7495,-18\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>7</title>\n",
       "<g id=\"a_node3\"><a xlink:title=\"W266\">\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"198,-36 144,-36 144,0 198,0 198,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">W266</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#cccccc\" d=\"M126.2813,-18C132.013,-18 137.7448,-18 143.4766,-18\"/>\n",
       "<polygon fill=\"#cccccc\" stroke=\"#cccccc\" points=\"143.7495,-18 143.7495,-18 143.7495,-18 143.7495,-18\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>9</title>\n",
       "<g id=\"a_node4\"><a xlink:title=\"ðŸ˜„\">\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"270,-36 216,-36 216,0 270,0 270,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-18.2844\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ðŸ˜„</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#cccccc\" d=\"M198.2813,-18C204.013,-18 209.7448,-18 215.4766,-18\"/>\n",
       "<polygon fill=\"#cccccc\" stroke=\"#cccccc\" points=\"215.7495,-18 215.7495,-18 215.7495,-18 215.7495,-18\"/>\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>0</title>\n",
       "<g id=\"a_node5\"><a xlink:title=\"&quot;(4&#10; &#160;(4 (2 I) (3 love) (3 W266))&#10; &#160;(3 ðŸ˜„))&quot;\">\n",
       "<ellipse fill=\"#052061\" stroke=\"#000000\" cx=\"164\" cy=\"-234\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"164\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#ffffff\">4</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1</title>\n",
       "<g id=\"a_node6\"><a xlink:title=\"&quot;(4 (2 I) (3 love) (3 W266))&quot;\">\n",
       "<ellipse fill=\"#052061\" stroke=\"#000000\" cx=\"137\" cy=\"-162\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"137\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#ffffff\">4</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M157.6014,-216.937C154.407,-208.4186 150.471,-197.9227 146.8982,-188.3952\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"150.1411,-187.0749 143.3527,-178.9405 143.5868,-189.5328 150.1411,-187.0749\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<g id=\"a_node10\"><a xlink:title=\"&quot;(3 ðŸ˜„)&quot;\">\n",
       "<ellipse fill=\"#92c5de\" stroke=\"#000000\" cx=\"209\" cy=\"-162\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"209\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M173.5433,-218.7307C179.4289,-209.3138 187.1086,-197.0263 193.8153,-186.2955\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"196.8273,-188.0801 199.1593,-177.7451 190.8913,-184.3701 196.8273,-188.0801\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2</title>\n",
       "<g id=\"a_node7\"><a xlink:title=\"&quot;(2 I)&quot;\">\n",
       "<ellipse fill=\"#f7f7f7\" stroke=\"#000000\" cx=\"45\" cy=\"-90\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"45\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M122.625,-150.75C107.7718,-139.1258 84.3924,-120.8288 67.1698,-107.3503\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"69.3062,-104.5778 59.2741,-101.171 64.992,-110.0904 69.3062,-104.5778\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>4</title>\n",
       "<g id=\"a_node8\"><a xlink:title=\"&quot;(3 love)&quot;\">\n",
       "<ellipse fill=\"#92c5de\" stroke=\"#000000\" cx=\"108\" cy=\"-90\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"108\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M130.2739,-145.3008C126.8464,-136.7912 122.5962,-126.2388 118.7301,-116.6403\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.8737,-115.0768 114.891,-107.1086 115.3806,-117.6922 121.8737,-115.0768\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6</title>\n",
       "<g id=\"a_node9\"><a xlink:title=\"&quot;(3 W266)&quot;\">\n",
       "<ellipse fill=\"#92c5de\" stroke=\"#000000\" cx=\"166\" cy=\"-90\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"166\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.7261,-145.3008C147.1536,-136.7912 151.4038,-126.2388 155.2699,-116.6403\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"158.6194,-117.6922 159.109,-107.1086 152.1263,-115.0768 158.6194,-117.6922\"/>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M40.5506,-72.2022C38.5751,-64.3005 36.2027,-54.811 33.999,-45.9959\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"37.3442,-44.9456 31.5233,-36.0931 30.5532,-46.6434 37.3442,-44.9456\"/>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M105.7289,-71.8314C104.7664,-64.131 103.6218,-54.9743 102.5521,-46.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0151,-45.9019 101.3017,-36.4133 99.0691,-46.7702 106.0151,-45.9019\"/>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M167.2617,-71.8314C167.7965,-64.131 168.4323,-54.9743 169.0266,-46.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"172.52,-46.6317 169.7213,-36.4133 165.5368,-46.1467 172.52,-46.6317\"/>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M213.1691,-144.3428C218.9493,-119.8619 229.4609,-75.342 236.3365,-46.222\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"239.7979,-46.7927 238.6896,-36.256 232.9852,-45.1841 239.7979,-46.7927\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "Tree('4', [Tree('4', [Tree('2', ['I']), Tree('3', ['love']), Tree('3', ['W266'])]), Tree('3', ['ðŸ˜„'])])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from w266_common import treeviz\n",
    "import sst\n",
    "# Monkey-patch NLTK with better Tree display that works on Cloud or other display-less server.\n",
    "print(\"Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\")\n",
    "treeviz.monkey_patch(nltk.tree.Tree, node_style_fn=sst.sst_node_style, format='svg')\n",
    "\n",
    "# Test rendering\n",
    "print(\"Sample tree to test rendering:\")\n",
    "nltk.tree.Tree.fromstring(\"(4 (4 (2 I) (3 love) (3 W266)) (3 ðŸ˜„))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishphilljoe/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"3ba5dbc4-4c3d-47ef-8992-289cca49f9ab\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"3ba5dbc4-4c3d-47ef-8992-289cca49f9ab\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"3ba5dbc4-4c3d-47ef-8992-289cca49f9ab\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3ba5dbc4-4c3d-47ef-8992-289cca49f9ab' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"3ba5dbc4-4c3d-47ef-8992-289cca49f9ab\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"3ba5dbc4-4c3d-47ef-8992-289cca49f9ab\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"3ba5dbc4-4c3d-47ef-8992-289cca49f9ab\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3ba5dbc4-4c3d-47ef-8992-289cca49f9ab' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"3ba5dbc4-4c3d-47ef-8992-289cca49f9ab\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK, NumPy, and Pandas.\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper libraries.\n",
    "from w266_common import utils, vocabulary, tf_embed_viz, treeviz\n",
    "from w266_common import patched_numpy_io\n",
    "# Code for this assignment\n",
    "import sst\n",
    "\n",
    "# Bokeh for plotting.\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()\n",
    "\n",
    "# Helper code for plotting histograms\n",
    "def plot_length_histogram(lengths, x_range=[0,100], bins=40, normed=True):\n",
    "    hist, bin_edges = np.histogram(a=lengths, bins=bins, normed=normed, range=x_range)\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "    bin_widths =  (bin_edges[1:] - bin_edges[:-1])\n",
    "\n",
    "    hover = HoverTool(tooltips=[(\"bucket\", \"@x\"), (\"count\", \"@top\")], mode=\"vline\")\n",
    "    fig = bp.figure(plot_width=800, plot_height=400, tools=[hover])\n",
    "    fig.vbar(x=bin_centers, width=bin_widths, top=hist, hover_fill_color=\"firebrick\")\n",
    "    fig.y_range.start = 0\n",
    "    fig.x_range.start = 0\n",
    "    fig.xaxis.axis_label = \"Example length (number of tokens)\"\n",
    "    fig.yaxis.axis_label = \"Frequency\"\n",
    "    bp.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (a): The Stanford Sentiment Treebank\n",
    "\n",
    "_If you haven't yet, be sure to familiarize yourself with the [Prelude notebook](Prelude.ipynb), as this assignment will assume familiarity with the text pre-processing steps described there._\n",
    "\n",
    "The [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) (SST) is one of the most widely used datasets as a benchmark for text classification. It consists of 11,855 sentences drawn from a corpus of movie reviews (originally from Rotten Tomatoes), each labeled with sentiment on a five-point scale.\n",
    "\n",
    "For example:\n",
    "```\n",
    "sentence: [A warm , funny , engaging film .]\n",
    "label:    4 (very positive)\n",
    "```\n",
    "\n",
    "For this assignment, we'll work with the binarized form of the dataset, where the lowest two classes are mapped to a single \"negative\" label, the highest two are mapped to a single \"positive\" label, and neutral examples are omitted.\n",
    "\n",
    "**Side note:**\n",
    "Unlike most classification datasets, SST is also a _treebank_, which means each sentence is associated with a tree structure that decomposes it into subphrases. So for the example above, we'd also have sentiment labels for `[warm , funny]` and `[engaging film .]` and so on. The trees are created by running the [Stanford Parser](https://nlp.stanford.edu/software/lex-parser.shtml) over the original sentences, then crowdsourcing sentiment labels on each sub-phrase. We'll talk more about treebanks later in the course, and in Assignment 5 you'll implement a simple version of the Stanford Parser.\n",
    "\n",
    "For the purposes of this assignment, we'll mostly concern ourselves with the sentence-level (\"root\") labels, but the tree structure will come in handy in two places:\n",
    "- As a way of analyzing the examples to find instances of negation\n",
    "- (optionally) As a source of additional training data, by including phrase labels\n",
    "\n",
    "### Obtaining the Data\n",
    "The data is distributed as serialized trees in [S-expression](https://en.wikipedia.org/wiki/S-expression) form, like this:\n",
    "```\n",
    "(4 (4 (2 A) (4 (3 (3 warm) (2 ,)) (3 funny))) (3 (2 ,) (3 (4 (4 engaging) (2 film)) (2 .))))\n",
    "```\n",
    "\n",
    "We've provided an `SSTDataset` class (in `sst.py`) which will download the dataset and parse the S-expressions into [`nltk.tree.Tree`](http://www.nltk.org/api/nltk.html?highlight=tree#nltk.tree.Tree) objects that you can easily view in the notebook.\n",
    "\n",
    "`SSTDataset` also implements the text-processing pipeline described in the [Prelude notebook](Prelude.ipynb), and provides methods (`as_sparse_bow` and `as_padded_array`) to convert the data to matrix form.\n",
    "\n",
    "Run the cell below; it will download a ~6MB .zip file to the local directory the first time you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading treebank to data/sst\n",
      "Loading SST from data/sst/trainDevTestTrees_PTB.zip\n",
      "Training set:     8,544 trees\n",
      "Development set:  1,101 trees\n",
      "Test set:         2,210 trees\n",
      "Building vocabulary - 16,474 words\n",
      "Processing to phrases...  Done!\n",
      "Splits: train / dev / test : 98,794 / 13,142 / 26,052\n"
     ]
    }
   ],
   "source": [
    "import sst\n",
    "ds = sst.SSTDataset(V=20000).process(label_scheme=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few members of the `SSTDataset()` class that you might find useful (click through and skim read):\n",
    "- **`ds.vocab`**: a [`vocabulary.Vocabulary`](https://github.com/datasci-w266/2019-summer-main/blob/master/common/vocabulary.py#L8) object managing the model vocabulary\n",
    "- **`ds.{train,dev,test}_trees`**: a list of [`nltk.tree.Tree`](http://www.nltk.org/api/nltk.html?highlight=tree#nltk.tree.Tree) objects representing each sentence\n",
    "- **`ds.{train,dev,test}`**: a Pandas DataFrame containing the _processed_ examples, including all subphrases. `label` is the target label, `is_root` denotes whether this example is a root node (full sentence), and `root_id` is the index of the tree that the example was derived from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('4', [Tree('4', [Tree('2', ['A']), Tree('4', [Tree('3', [Tree('3', ['warm']), Tree('2', [','])]), Tree('3', ['funny'])])]), Tree('3', [Tree('2', [',']), Tree('3', [Tree('4', [Tree('4', ['engaging']), Tree('2', ['film'])]), Tree('2', ['.'])])])])]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at a tree for a positive review\n",
    "ds.dev_trees[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('0', [Tree('2', ['It']), Tree('2', [Tree('2', [Tree('2', [\"'s\"]), Tree('0', [Tree('2', ['like']), Tree('1', [Tree('2', ['watching']), Tree('0', [Tree('1', [Tree('2', ['a']), Tree('0', ['nightmare'])]), Tree('2', [Tree('2', ['made']), Tree('2', ['flesh'])])])])])]), Tree('2', ['.'])])])]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at a tree for a negative review\n",
    "ds.dev_trees[361:362]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "4     1\n",
       "5     1\n",
       "7     1\n",
       "25    1\n",
       "31    1\n",
       "33    1\n",
       "35    1\n",
       "37    1\n",
       "39    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels.\n",
    "ds.train.label[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) questions: Exploring the Data\n",
    "\n",
    "Let's explore the data a bit, to get a sense of what we're working with. If you're not familiar with DataFrames, you may wish to review the Pandas documentation: https://pandas.pydata.org/pandas-docs/stable/dsintro.html \n",
    "\n",
    "Answer the following questions in the cells below:\n",
    "\n",
    "1. Looking at only the root examples in the training set (*Hint: use `ds.train[ds.train.is_root]`*), what is the fraction of positive labels? Is the classification task balanced, or close to it? If we used most-common-class as a baseline classifier, what would the accuracy be?\n",
    "2. What are the five most common tokens (excluding punctuation) in the dataset? (*Hint: there are several ways to get at this - you might use `collections.Counter`, or poke around in the `ds.vocab` object.*)\n",
    "3. Use the `plot_length_histogram` function (defined above) to plot a histogram of the sentence lengths. What is the 95% percentile length? (i.e. 95% of examples in the training set should be shorter than this)\n",
    "4. Repeat 3., but this time including all subphrases. Notice the difference in distributions. Think about how predicting subphrases might be problematic if they're too short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cells below for your code solutions. Each part shouldn't require more than a couple lines, but you're welcome to explore more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Positive samples: 3610\n",
      "Count of negative sampels: 3310\n",
      "The ratio of positive samples: 0.5216763005780347\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (a).1\n",
    "root_samples = ds.train[(ds.train.is_root)]\n",
    "pos_sample_count = len(root_samples[root_samples['label']==1])\n",
    "neg_sample_count = len(root_samples[root_samples['label']==0])\n",
    "print('Count of Positive samples: '+str(pos_sample_count)+'\\nCount of negative sampels: '+str(neg_sample_count)+\n",
    "     '\\nThe ratio of positive samples: '+str(pos_sample_count/len(root_samples)))\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 5954), ('a', 4361), ('and', 3831), ('of', 3631), ('to', 2438)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (a).2\n",
    "corpus = []\n",
    "#print(root_samples['tokens'].iloc[0])\n",
    "for i in range(0,len(root_samples)):\n",
    "    corpus.extend(root_samples['tokens'].iloc[i])\n",
    "    \n",
    "counted_corpus = collections.Counter(corpus)\n",
    "sorted_corpus = sorted(counted_corpus.items(), key=lambda x: x[1],reverse=True)\n",
    "sorted_corpus = list(filter(lambda x: x[0] not in ('.',','), sorted_corpus))\n",
    "sorted_corpus[0:5]\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% percentile length is: 36 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishphilljoe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"fce79352-9cbc-4f32-8b85-5160377b061c\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"77eb2337-7c91-4e6b-a3b1-94bd06eb3fa3\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"2bba332d-fa99-4ba3-9d86-e0f86848c033\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"b690504d-8cc2-46af-9f18-ec3e236425f4\",\"type\":\"HoverTool\"}]},\"id\":\"27b6ae2c-aefd-4d62-9593-e3d763a85415\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"d612ab95-4b8a-4787-9af1-a01709094169\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"f42358aa-22d4-434e-a482-a7c7182d2085\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"plot\":{\"id\":\"e503c556-4973-4e5e-9aeb-2a4cf7c830d1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"dfda0d75-a103-4d1d-ae03-5db6d7885a28\",\"type\":\"BasicTicker\"}},\"id\":\"313a983d-207d-4f02-81b2-5ecb2e6ea5a8\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"988546b6-b2e9-4e38-aae6-3780dd95cdcf\",\"type\":\"Selection\"},{\"attributes\":{\"data_source\":{\"id\":\"37add316-99df-4823-991d-d9089b601c49\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5e4bec1f-d84b-4463-a669-21ab2f1d328e\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"4f597c7f-e768-4f48-98fe-8ffa5e5b97dd\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"016e1a3a-c696-43b8-a953-eae213b289a9\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"35b9c298-8b6b-44bb-9017-0930dd10354a\",\"type\":\"CDSView\"}},\"id\":\"c959908d-c7df-4cb0-bf5a-0fbae501df15\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"dfda0d75-a103-4d1d-ae03-5db6d7885a28\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis_label\":\"Example length (number of tokens)\",\"formatter\":{\"id\":\"a4c7baab-9336-486c-b1af-150c889729ab\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"e503c556-4973-4e5e-9aeb-2a4cf7c830d1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"dfda0d75-a103-4d1d-ae03-5db6d7885a28\",\"type\":\"BasicTicker\"}},\"id\":\"e3a4d07f-f82b-4ac9-9402-43f8fb1b3454\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":{\"__ndarray__\":\"w+zgCCKtVz8DAoYIsLqGP4B2lqrmO5w/b5vmKEm8lz+HOOFnQQOlPxSIoYsVBJ8/M5JSiTDcqD/YWGeWlxOhP1dTgOiIq6Y/1MeQir8snD+GjXVmeTmhPwdjK2ddHpM/W1+RSP7Ylj9zp/eIvumHP1NHb4gTfoY/Y8xHByFqcz8Th16HvaZzP8SXTArqdls/hIKnClxpXD+EgqcKXGlMPwNXGgfo8EI/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"shape\":[40]},\"width\":{\"__ndarray__\":\"AAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEA=\",\"dtype\":\"float64\",\"shape\":[40]},\"x\":{\"__ndarray__\":\"AAAAAAAA9D8AAAAAAAAOQAAAAAAAABlAAAAAAACAIUAAAAAAAIAmQAAAAAAAgCtAAAAAAABAMEAAAAAAAMAyQAAAAAAAQDVAAAAAAADAN0AAAAAAAEA6QAAAAAAAwDxAAAAAAABAP0AAAAAAAOBAQAAAAAAAIEJAAAAAAABgQ0AAAAAAAKBEQAAAAAAA4EVAAAAAAAAgR0AAAAAAAGBIQAAAAAAAoElAAAAAAADgSkAAAAAAACBMQAAAAAAAYE1AAAAAAACgTkAAAAAAAOBPQAAAAAAAkFBAAAAAAAAwUUAAAAAAANBRQAAAAAAAcFJAAAAAAAAQU0AAAAAAALBTQAAAAAAAUFRAAAAAAADwVEAAAAAAAJBVQAAAAAAAMFZAAAAAAADQVkAAAAAAAHBXQAAAAAAAEFhAAAAAAACwWEA=\",\"dtype\":\"float64\",\"shape\":[40]}},\"selected\":{\"id\":\"988546b6-b2e9-4e38-aae6-3780dd95cdcf\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"f42358aa-22d4-434e-a482-a7c7182d2085\",\"type\":\"UnionRenderers\"}},\"id\":\"37add316-99df-4823-991d-d9089b601c49\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"016e1a3a-c696-43b8-a953-eae213b289a9\",\"type\":\"VBar\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"213a857d-0fa6-4419-a776-65ca7bb6cec0\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"56b02886-2c9c-47ae-81e9-e875d339439d\",\"type\":\"LinearScale\"},{\"attributes\":{\"below\":[{\"id\":\"e3a4d07f-f82b-4ac9-9402-43f8fb1b3454\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"c89a2d99-edf2-4912-b926-b9c09f24174e\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"e3a4d07f-f82b-4ac9-9402-43f8fb1b3454\",\"type\":\"LinearAxis\"},{\"id\":\"313a983d-207d-4f02-81b2-5ecb2e6ea5a8\",\"type\":\"Grid\"},{\"id\":\"c89a2d99-edf2-4912-b926-b9c09f24174e\",\"type\":\"LinearAxis\"},{\"id\":\"2bcff6bc-6d7b-490b-acbb-eb7247c850d2\",\"type\":\"Grid\"},{\"id\":\"c959908d-c7df-4cb0-bf5a-0fbae501df15\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"dff19d51-1e48-4df7-830b-0670a440948e\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"27b6ae2c-aefd-4d62-9593-e3d763a85415\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"213a857d-0fa6-4419-a776-65ca7bb6cec0\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"2e2491ff-9556-4701-895c-8c3057455e84\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"ebe73341-5719-4a9a-a5bb-0ad5de4be34c\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"56b02886-2c9c-47ae-81e9-e875d339439d\",\"type\":\"LinearScale\"}},\"id\":\"e503c556-4973-4e5e-9aeb-2a4cf7c830d1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":\"auto\",\"tooltips\":[[\"bucket\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"b690504d-8cc2-46af-9f18-ec3e236425f4\",\"type\":\"HoverTool\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"e503c556-4973-4e5e-9aeb-2a4cf7c830d1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"d612ab95-4b8a-4787-9af1-a01709094169\",\"type\":\"BasicTicker\"}},\"id\":\"2bcff6bc-6d7b-490b-acbb-eb7247c850d2\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"2e2491ff-9556-4701-895c-8c3057455e84\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis_label\":\"Frequency\",\"formatter\":{\"id\":\"2bba332d-fa99-4ba3-9d86-e0f86848c033\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"e503c556-4973-4e5e-9aeb-2a4cf7c830d1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"d612ab95-4b8a-4787-9af1-a01709094169\",\"type\":\"BasicTicker\"}},\"id\":\"c89a2d99-edf2-4912-b926-b9c09f24174e\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"dff19d51-1e48-4df7-830b-0670a440948e\",\"type\":\"Title\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"5e4bec1f-d84b-4463-a669-21ab2f1d328e\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"a4c7baab-9336-486c-b1af-150c889729ab\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"4f597c7f-e768-4f48-98fe-8ffa5e5b97dd\",\"type\":\"VBar\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"ebe73341-5719-4a9a-a5bb-0ad5de4be34c\",\"type\":\"DataRange1d\"},{\"attributes\":{\"source\":{\"id\":\"37add316-99df-4823-991d-d9089b601c49\",\"type\":\"ColumnDataSource\"}},\"id\":\"35b9c298-8b6b-44bb-9017-0930dd10354a\",\"type\":\"CDSView\"}],\"root_ids\":[\"e503c556-4973-4e5e-9aeb-2a4cf7c830d1\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.16\"}};\n",
       "  var render_items = [{\"docid\":\"77eb2337-7c91-4e6b-a3b1-94bd06eb3fa3\",\"elementid\":\"fce79352-9cbc-4f32-8b85-5160377b061c\",\"modelid\":\"e503c556-4973-4e5e-9aeb-2a4cf7c830d1\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "e503c556-4973-4e5e-9aeb-2a4cf7c830d1"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (a).3\n",
    "#root_samples['tokens'].iloc[1]\n",
    "sample_len = []\n",
    "for i in range(0,len(root_samples)):\n",
    "    sample_len.append(len(root_samples['tokens'].iloc[i]))\n",
    "print('95% percentile length is: '+str(int(np.percentile(sample_len, 95)))+' tokens')\n",
    "plot_length_histogram(sample_len, x_range=[0,100], bins=40, normed=True)\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% percentile length is: 24 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishphilljoe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"a1a05db4-d5f1-4caf-b9e3-844ced2963fe\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"a6596083-1dae-40e8-a7be-85c1900b3517\":{\"roots\":{\"references\":[{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"7e9815e7-081e-495a-8d81-23cb83c122ae\",\"type\":\"VBar\"},{\"attributes\":{\"plot\":{\"id\":\"0f73c7d1-7d80-4614-8782-6715b41bbca5\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"4ec130df-1b73-4e7d-aec4-dc5d8713c8bc\",\"type\":\"BasicTicker\"}},\"id\":\"55309f24-a320-49c1-8c40-d5b3f486dce7\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"4ec130df-1b73-4e7d-aec4-dc5d8713c8bc\",\"type\":\"BasicTicker\"},{\"attributes\":{\"source\":{\"id\":\"65c9b6dd-9dd1-4cd8-a2e4-484896ae5e68\",\"type\":\"ColumnDataSource\"}},\"id\":\"34623f7a-598f-43ca-9296-823a8fae5664\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"b73fac08-b5d2-478f-9b19-af544c29154b\",\"type\":\"Selection\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"e0708dde-c9ca-4e5a-9961-4dc57a43b2f0\",\"type\":\"VBar\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"43b7f051-3736-4e53-beb6-c22fb56a79de\",\"type\":\"DataRange1d\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"4a08598b-b27b-4c84-b60e-63d528387937\",\"type\":\"HoverTool\"}]},\"id\":\"33e655c4-eab5-4ede-9de2-42b26d9c5f49\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"ec03bb9f-16dd-43a3-ab86-2171cd30814d\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"8679d549-6589-40d2-93fe-a1030fdbd0d8\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"be589faa-4bb6-413c-bd0b-dd88f3c1b7d7\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"below\":[{\"id\":\"8aa96320-2e5e-46b6-b349-04e2dd6ac728\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"aeb59b92-ab30-4d50-9a20-a237ebcb0b07\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"8aa96320-2e5e-46b6-b349-04e2dd6ac728\",\"type\":\"LinearAxis\"},{\"id\":\"55309f24-a320-49c1-8c40-d5b3f486dce7\",\"type\":\"Grid\"},{\"id\":\"aeb59b92-ab30-4d50-9a20-a237ebcb0b07\",\"type\":\"LinearAxis\"},{\"id\":\"5d84bf65-0753-41de-b4a8-4a079f6b2c03\",\"type\":\"Grid\"},{\"id\":\"c53ea4a7-0228-4006-81a6-e18146481c63\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"5fe5c06b-47b6-47b0-983f-c415ff5db11a\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"33e655c4-eab5-4ede-9de2-42b26d9c5f49\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"43b7f051-3736-4e53-beb6-c22fb56a79de\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"2ee59c10-d616-4395-bf1f-57b4dc67a87f\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"45be5b7e-821f-46e6-b26a-881a2256c32b\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"eb63eabc-6bde-42b7-a6f8-e6cfc5f27ce9\",\"type\":\"LinearScale\"}},\"id\":\"0f73c7d1-7d80-4614-8782-6715b41bbca5\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"eb63eabc-6bde-42b7-a6f8-e6cfc5f27ce9\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":\"auto\",\"tooltips\":[[\"bucket\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"4a08598b-b27b-4c84-b60e-63d528387937\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"98f790fc-f8d0-487b-ad77-04006004e624\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"5fe5c06b-47b6-47b0-983f-c415ff5db11a\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"65c9b6dd-9dd1-4cd8-a2e4-484896ae5e68\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"7e9815e7-081e-495a-8d81-23cb83c122ae\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"e0708dde-c9ca-4e5a-9961-4dc57a43b2f0\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"8679d549-6589-40d2-93fe-a1030fdbd0d8\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"34623f7a-598f-43ca-9296-823a8fae5664\",\"type\":\"CDSView\"}},\"id\":\"c53ea4a7-0228-4006-81a6-e18146481c63\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"axis_label\":\"Frequency\",\"formatter\":{\"id\":\"98f790fc-f8d0-487b-ad77-04006004e624\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"0f73c7d1-7d80-4614-8782-6715b41bbca5\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"8285bd6c-3aae-4d12-aba5-932524c395c1\",\"type\":\"BasicTicker\"}},\"id\":\"aeb59b92-ab30-4d50-9a20-a237ebcb0b07\",\"type\":\"LinearAxis\"},{\"attributes\":{\"axis_label\":\"Example length (number of tokens)\",\"formatter\":{\"id\":\"be589faa-4bb6-413c-bd0b-dd88f3c1b7d7\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"0f73c7d1-7d80-4614-8782-6715b41bbca5\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"4ec130df-1b73-4e7d-aec4-dc5d8713c8bc\",\"type\":\"BasicTicker\"}},\"id\":\"8aa96320-2e5e-46b6-b349-04e2dd6ac728\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"2ee59c10-d616-4395-bf1f-57b4dc67a87f\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":{\"__ndarray__\":\"nMUkk8aSwT+cE1gJAVivP0XlzTOkzK0/aeCda4W+nD9JZJCtxIigPyKIRrBMHJE/pUYd02DRkz+E+zxFkPqEP/27SEuPOIg/Ee17Rm4oeT8rMSIEDH97P4KaFIqu/2o/jE+Gu49JbT9AMcvjZA9cP8Mw1aVPrVg/KSHWa1H2RD9rih8Sm+ZDPyjI0/zMPy4/C+ZBdjuoLD/ZxkMCPzoVP58CIPUbCxI/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"shape\":[40]},\"width\":{\"__ndarray__\":\"AAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEA=\",\"dtype\":\"float64\",\"shape\":[40]},\"x\":{\"__ndarray__\":\"AAAAAAAA9D8AAAAAAAAOQAAAAAAAABlAAAAAAACAIUAAAAAAAIAmQAAAAAAAgCtAAAAAAABAMEAAAAAAAMAyQAAAAAAAQDVAAAAAAADAN0AAAAAAAEA6QAAAAAAAwDxAAAAAAABAP0AAAAAAAOBAQAAAAAAAIEJAAAAAAABgQ0AAAAAAAKBEQAAAAAAA4EVAAAAAAAAgR0AAAAAAAGBIQAAAAAAAoElAAAAAAADgSkAAAAAAACBMQAAAAAAAYE1AAAAAAACgTkAAAAAAAOBPQAAAAAAAkFBAAAAAAAAwUUAAAAAAANBRQAAAAAAAcFJAAAAAAAAQU0AAAAAAALBTQAAAAAAAUFRAAAAAAADwVEAAAAAAAJBVQAAAAAAAMFZAAAAAAADQVkAAAAAAAHBXQAAAAAAAEFhAAAAAAACwWEA=\",\"dtype\":\"float64\",\"shape\":[40]}},\"selected\":{\"id\":\"b73fac08-b5d2-478f-9b19-af544c29154b\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"ec03bb9f-16dd-43a3-ab86-2171cd30814d\",\"type\":\"UnionRenderers\"}},\"id\":\"65c9b6dd-9dd1-4cd8-a2e4-484896ae5e68\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"45be5b7e-821f-46e6-b26a-881a2256c32b\",\"type\":\"DataRange1d\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"0f73c7d1-7d80-4614-8782-6715b41bbca5\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"8285bd6c-3aae-4d12-aba5-932524c395c1\",\"type\":\"BasicTicker\"}},\"id\":\"5d84bf65-0753-41de-b4a8-4a079f6b2c03\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"8285bd6c-3aae-4d12-aba5-932524c395c1\",\"type\":\"BasicTicker\"}],\"root_ids\":[\"0f73c7d1-7d80-4614-8782-6715b41bbca5\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.16\"}};\n",
       "  var render_items = [{\"docid\":\"a6596083-1dae-40e8-a7be-85c1900b3517\",\"elementid\":\"a1a05db4-d5f1-4caf-b9e3-844ced2963fe\",\"modelid\":\"0f73c7d1-7d80-4614-8782-6715b41bbca5\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "0f73c7d1-7d80-4614-8782-6715b41bbca5"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (a).4\n",
    "sample_len = []\n",
    "for i in range(0,len(ds.train)):\n",
    "    sample_len.append(len(ds.train['tokens'].iloc[i]))\n",
    "print('95% percentile length is: '+str(int(np.percentile(sample_len, 95)))+' tokens')\n",
    "plot_length_histogram(sample_len, x_range=[0,100], bins=40, normed=True)\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (b): Naive Bayes\n",
    "\n",
    "In this section, we'll build and explore a Naive Bayes model as a baseline classifier for our dataset.\n",
    "\n",
    "Naive Bayes is perhaps the simplest possible classification algorithm, but it's one that still surprisingly effective for many text classification problems. Recall from your Machine Learning course:\n",
    "\n",
    "$$ P(y = k) = \\hat{\\theta}_k = \\frac{1}{N}\\sum_{i = 1}^N \\mathbf{1}[y_i = k] $$\n",
    "\n",
    "$$ P(x_j | y = 1) = \\hat{\\theta}_{k,j} = \n",
    "\\frac{ \n",
    "\\sum_{i = 1}^N  \\sum_{j' = 1}^{n_i} \\mathbf{1}[y_i = 1 \\wedge x_{j'} = j]\n",
    "}{\n",
    "\\sum_{i = 1}^N  \\mathbf{1}[y_i = 1] \\cdot n_i\n",
    "}\n",
    "$$\n",
    "\n",
    "where $N$ is the size of the dataset, and $n_i$ is the length (number of tokens of the $i^{th}$ example. Prediction is done by computing the score:\n",
    "\n",
    "$$ \\mathrm{score}(x) = \\log \\left(\\frac{P(y = 1) \\prod_{j=1}^n P(x_j | y = 1)}{P(y = 0) \\prod_{j=1}^n P(x_j | y = 0)}\\right) $$\n",
    "\n",
    "We'll just use the [implementation from scikit-learn](http://scikit-learn.org/stable/modules/naive_bayes.html). Like other scikit-learn classifiers, this expects the input as a `scipy.sparse` matrix. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: x = (6920, 16474) sparse, y = (6920,)\n",
      "Test set:     x = (1821, 16474) sparse, y = (1821,)\n"
     ]
    }
   ],
   "source": [
    "# 'csr' stands for \"Compressed Sparse Row\", which is one format\n",
    "# for representing sparse matricies.\n",
    "train_x_csr, train_y = ds.as_sparse_bow(\"train\", root_only=True)\n",
    "test_x_csr,  test_y  = ds.as_sparse_bow(\"test\", root_only=True)\n",
    "print(\"Training set: x = {:s} sparse, y = {:s}\".format(str(train_x_csr.shape), \n",
    "                                                str(train_y.shape)))\n",
    "print(\"Test set:     x = {:s} sparse, y = {:s}\".format(str(test_x_csr.shape), \n",
    "                                                str(test_y.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `root_only=True` parameter - this will return only examples corresponding to whole sentences. If you set this to false, you can get examples for all phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b) Questions\n",
    "\n",
    "Answer the following questions in the **answer file found in this directory** and code cells below.\n",
    "\n",
    "**Question 1.)** Implement Naive Bayes using `sklearn.naive_bayes.MultinomialNB`. Train on the training set and evaluate accuracy on the test set using `.predict(...)`. \n",
    "\n",
    "Your model should train almost instantly, and score between 82% and 83% - not bad! On SST, this is actually a very strong baseline.\n",
    "\n",
    "Recall that Naive Bayes can be interpreted as a linear model, where score is given by:\n",
    "\n",
    "$$ \\mathrm{score}(x) = \\log \\left(\\frac{P(y = 1) \\prod_{j=1}^n P(x_j | y = 1)}{P(y = 0) \\prod_{j=1}^n P(x_j | y = 0)}\\right) \n",
    "= \\left( \\log\\hat{\\theta}_1 - \\log\\hat{\\theta}_0 \\right) + \\sum_{j=1}^n \\left( \\log\\hat{\\theta}_{1,j} - \\log\\hat{\\theta}_{0,j} \\right)$$\n",
    "\n",
    "You can access the values $\\log\\hat{\\theta}_{k,j}$ from the trained model using `nb.feature_log_prob_[k,j]`.\n",
    "\n",
    "**Question 2.)** In the cell below, compute the weights $w_j = \\left( \\log\\hat{\\theta}_{1,j} - \\log\\hat{\\theta}_{0,j} \\right)$ of the linear model, and find the top 10 most negative and most positive weights. _(Hint: use `np.argsort` to get the indices of the most extreme elements.)_ Put the most negative and most positive word in the answers file.  Do the features you found make sense for this domain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 82.21%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "#### YOUR CODE HERE ####\n",
    "# Code for part (b).1\n",
    "nb = MultinomialNB()\n",
    "y_pred = nb.fit(train_x_csr, train_y).predict(test_x_csr)\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "acc = accuracy_score(test_y, y_pred)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative features:\n",
      "  stupid (-3.17)\n",
      "  suffers (-3.08)\n",
      "  unfunny (-2.97)\n",
      "  mess (-2.79)\n",
      "  pointless (-2.79)\n",
      "  flat (-2.75)\n",
      "  poorly (-2.72)\n",
      "  car (-2.72)\n",
      "  tiresome (-2.64)\n",
      "  disguise (-2.56)\n",
      "\n",
      "Most positive features:\n",
      "  powerful (3.53)\n",
      "  solid (3.45)\n",
      "  perfectly (2.81)\n",
      "  inventive (2.69)\n",
      "  refreshing (2.63)\n",
      "  touching (2.56)\n",
      "  wonderful (2.49)\n",
      "  riveting (2.49)\n",
      "  portrait (2.41)\n",
      "  thought-provoking (2.41)\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for part (b).2\n",
    "linear_weights = None  # populate this with actual values\n",
    " \n",
    "weights = nb.feature_log_prob_\n",
    "\n",
    "#print(weights[0])\n",
    "#print(weights[1])\n",
    "linear_weights = weights[1]-weights[0]\n",
    "top_negative_features = np.argsort(weights[1]-weights[0])[0:10]\n",
    "top_positive_features = np.argsort(weights[0]-weights[1])[0:10]\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "\n",
    "print(\"Most negative features:\")\n",
    "for idx in top_negative_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(ds.vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))\n",
    "print(\"\")\n",
    "print(\"Most positive features:\")\n",
    "for idx in top_positive_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(ds.vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (c): Examining Negation\n",
    "\n",
    "While Naive Bayes performs well in the aggregate, as a linear model it's still limited in its ability to model complex phenomena in the data. Each feature - in this case, each word - contributes a weight to the total, and if the sum is $\\ge 0$ we predict the example is positive. But what happens when we have an example with both positive and negative words? For instance:\n",
    "\n",
    "```\n",
    "[Brando 's performance fell short of the high standards set by his earlier work .]\n",
    "[A thoughtful look at a painful incident that made headlines in 1995 .]\n",
    "```\n",
    "\n",
    "Run the cell below to evaluate your model on these examples. It should predict both as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1], dtype=int32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\"Brando 's performance fell short of the high standards set by his earlier work .\",\n",
    "            \"A thoughtful look at a painful incident that made headlines in 1995 .\"]\n",
    "canonicalized_examples = [ds.canonicalize(s.split()) for s in examples]\n",
    "id_lists = [ds.vocab.words_to_ids(s) for s in canonicalized_examples]\n",
    "x = utils.id_lists_to_sparse_bow(id_lists, ds.vocab.size)\n",
    "nb.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c).1\n",
    "<a id=\"answers_c1\"></a>\n",
    "\n",
    "**Question 1.)** Why does the model get the first example wrong?  Think about what other kinds of sentence structure might fail to be properly understood by a linear model.  *Hint: See the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<unk>', -0.07947581123340974),\n",
       " (\"'s\", 0.032134724769587564),\n",
       " ('performance', 1.0991791851082366),\n",
       " ('fell', -0.7726229917933551),\n",
       " ('short', -0.5213085635124504),\n",
       " ('of', 0.1577996227186711),\n",
       " ('the', 0.035492833505731625),\n",
       " ('high', -0.07947581123341152),\n",
       " ('standards', 0.6136713693265357),\n",
       " ('set', -0.18483632689123652),\n",
       " ('by', -0.03560912912782843),\n",
       " ('his', 0.15123398046411562),\n",
       " ('earlier', -0.995766543107564),\n",
       " ('work', 0.4801399767020129),\n",
       " ('.', 0.0184617237532958)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in zip(canonicalized_examples[0], linear_weights[np.array(id_lists[0])])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always important to look at individual examples, but let's try to do this a bit more systematically. Recall that SST gives us labels not only at the whole-sentence (root) level, but for individual phrases as well. We can use this to look for examples where polarity changes between different parts of the sentence. Here's one of the examples above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('3', [Tree('4', [Tree('2', ['A']), Tree('4', ['thoughtful'])]), Tree('1', [Tree('1', [Tree('2', ['look']), Tree('2', [Tree('2', ['at']), Tree('2', [Tree('1', [Tree('2', ['a']), Tree('1', [Tree('1', ['painful']), Tree('2', ['incident'])])]), Tree('2', [Tree('2', ['that']), Tree('2', [Tree('3', [Tree('2', ['made']), Tree('2', ['headlines'])]), Tree('2', [Tree('2', ['in']), Tree('2', ['1995'])])])])])])]), Tree('2', ['.'])])])]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.test_trees[210:211]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will comb through the test set, looking for examples where there's some degree of ambiguity. We'll use a fairly crude heuristic for now: count up all the non-neutral phrases for a given sentence, and look at ones where there's a mix of both positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 246 interesting examples\n",
      "Interesting ids (into ds.test_trees):  [0, 27, 31, 32, 75, 80, 90, 96, 117, 124, 138, 140, 141, 160, 166, 186, 187, 205, 210, 212, 227, 232, 254, 269, 271, 285, 296, 307, 312, 327, 335, 373, 397, 399, 406, 407, 410, 426, 447, 511, 512, 516, 521, 534, 539, 563, 577, 588, 606, 610, 611, 637, 640, 645, 655, 662, 664, 713, 720, 721, 724, 739, 755, 758, 763, 776, 791, 793, 796, 802, 805, 810, 818, 840, 858, 887, 898, 899, 909, 910, 912, 929, 930, 961, 970, 973, 974, 975, 979, 1008, 1032, 1036, 1066, 1067, 1076, 1098, 1101, 1108, 1114, 1131, 1138, 1142, 1159, 1183, 1185, 1189, 1193, 1198, 1206, 1214, 1215, 1235, 1241, 1243, 1244, 1261, 1267, 1273, 1275, 1279, 1280, 1293, 1296, 1302, 1303, 1312, 1318, 1319, 1321, 1322, 1324, 1326, 1328, 1338, 1341, 1346, 1359, 1363, 1371, 1383, 1398, 1402, 1413, 1443, 1452, 1456, 1458, 1462, 1464, 1480, 1481, 1486, 1487, 1488, 1507, 1509, 1513, 1516, 1527, 1537, 1552, 1576, 1582, 1587, 1594, 1597, 1602, 1607, 1608, 1615, 1619, 1622, 1629, 1630, 1639, 1666, 1682, 1688, 1694, 1727, 1728, 1731, 1755, 1763, 1764, 1786, 1789, 1793, 1795, 1798, 1804, 1807, 1817, 1830, 1834, 1850, 1852, 1883, 1885, 1899, 1903, 1908, 1910, 1918, 1929, 1933, 1939, 1940, 1942, 1943, 1945, 1947, 1949, 1950, 1952, 1954, 1964, 1965, 1970, 1972, 1980, 1982, 2008, 2009, 2011, 2021, 2035, 2036, 2038, 2063, 2079, 2085, 2089, 2094, 2096, 2118, 2119, 2121, 2143, 2145, 2149, 2150, 2160, 2164, 2190, 2193]\n"
     ]
    }
   ],
   "source": [
    "df = ds.test\n",
    "\n",
    "gb = df.groupby(by=['root_id'])\n",
    "interesting_ids = []   # root ids, index into ds.test_trees\n",
    "interesting_idxs = []  # DataFrame indices, index into ds.test\n",
    "# This groups the DataFrame by sentence\n",
    "for root_id, idxs in gb.groups.items():\n",
    "    # Get the average score of all the phrases for this sentence\n",
    "    mean = df.loc[idxs].label.mean()\n",
    "    if (mean > 0.4 and mean < 0.6):\n",
    "        interesting_ids.append(root_id)\n",
    "        interesting_idxs.extend(idxs)\n",
    "        \n",
    "print(\"Found {:,} interesting examples\".format(len(interesting_ids)))\n",
    "# This will extract only the \"interesting\" sentences we found above\n",
    "test_x_interesting, test_y_interesting = ds.as_sparse_bow(\"test\", root_only=True, \n",
    "                                                          df_idxs=interesting_idxs)\n",
    "print(\"Interesting ids (into ds.test_trees): \", interesting_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c) continued\n",
    "\n",
    "Answer the following in the cells below.\n",
    "\n",
    "**Question 2.)** Examine a few of the \"interesting\" trees. What kinds of patterns do you see? What is the relation of the polarity of the sub-phrases to that of the whole sentence? Is this well-captured by a linear model?\n",
    "\n",
    "**Question 3.)** Evaluate your model on `test_x_interesting` and compute accuracy. Does your model do better or worse on this slice of the data than on the whole test set (interesting + uninteresting examples)? What is the _relative_ change in the error rate? _(For example, if you go from 90% accuracy to 85%, that's a 50% increase in error!)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                   3                                                                                \n",
      "  _________________________________________________________________________________|_______                                                                          \n",
      " |                                                                                         3                                                                        \n",
      " |             ____________________________________________________________________________|______________________________________________________________________   \n",
      " |            3                                                                                                                                                   | \n",
      " |    ________|_______________                                                                                                                                    |  \n",
      " |   |                        2                                                                                                                                   | \n",
      " |   |         _______________|________________                                                                                                                   |  \n",
      " |   |        |                                2                                                                                                                  | \n",
      " |   |        |                ________________|___________________________________                                                                               |  \n",
      " |   |        |               |                                                    3                                                                              | \n",
      " |   |        |               |                              ______________________|_________________                                                             |  \n",
      " |   |        |               |                             |                                        2                                                            | \n",
      " |   |        |               |                             |                                    ____|__________                                                  |  \n",
      " |   |        |               |                             |                                   |               1                                                 | \n",
      " |   |        |               |                             |                                   |     __________|_____________                                    |  \n",
      " |   |        |               |                             3                                   |    |                        1                                   | \n",
      " |   |        |               |                 ____________|______________________________     |    |     ___________________|_____                              |  \n",
      " |   |        |               |                3                                           |    |    |    |                         1                             | \n",
      " |   |        |               |      __________|____________                               |    |    |    |          _______________|_______                      |  \n",
      " |   |        |               |     |                       2                              |    |    |    |         |                       2                     | \n",
      " |   |        |               |     |     __________________|______                        |    |    |    |         |                _______|_____                |  \n",
      " |   |        2               |     |    |                         4                       |    |    |    |         1               |             2               | \n",
      " |   |    ____|____           |     |    |            _____________|________               |    |    |    |      ___|___            |    _________|_____          |  \n",
      " |   |   |         2          |     |    |           3                      1              |    |    |    |     |       1           |   |               2         | \n",
      " |   |   |     ____|____      |     |    |      _____|______            ____|______        |    |    |    |     |    ___|_____      |   |          _____|____     |  \n",
      " 2   2   2    2         3     2     2    2     2            3          2           2       2    2    2    2     2   1         2     2   2         2          2    2 \n",
      " |   |   |    |         |     |     |    |     |            |          |           |       |    |    |    |     |   |         |     |   |         |          |    |  \n",
      " It  's  a  minor     comedy that tries  to balance     sweetness     with     coarseness  ,  while  it paints  a  sad     picture  of the     singles     scene  . \n",
      "\n",
      "                                                                                                              3                                                                                                                   \n",
      "       _______________________________________________________________________________________________________|___________________________                                                                                         \n",
      "      |                                                                                                                                   2                                                                                       \n",
      "      |                                     ______________________________________________________________________________________________|______                                                                                  \n",
      "      |                                    |                                                                                                     2                                                                                \n",
      "      |                                    |                  ___________________________________________________________________________________|_____________                                                                    \n",
      "      |                                    |                 |                                                                                                 3                                                                  \n",
      "      |                                    |                 |                                                                                           ______|________________________________________________________________   \n",
      "      |                                    |                 |                                                                                          2                                                                       | \n",
      "      |                                    |                 |                            ______________________________________________________________|______                                                                 |  \n",
      "      1                                    |                 |                           |                                                                     3                                                                | \n",
      "  ____|________                            |                 |                           |                                                 ____________________|____________________________________                            |  \n",
      " |             1                           |                 |                           |                                                3                                                         |                           | \n",
      " |     ________|_____                      |                 |                           |                              __________________|____________________                                     |                           |  \n",
      " |    |              2                     |                 |                           |                             |                                       2                                    1                           | \n",
      " |    |     _________|_____                |                 |                           |                             |                          _____________|___                 ________________|____                       |  \n",
      " |    |    |               2               |                 |                           |                             3                         |                 2               |                     1                      | \n",
      " |    |    |          _____|___________    |                 |                           |               ______________|____                     |       __________|___            |            _________|_______               |  \n",
      " |    |    |         2                 |   |                 2                           |              |                   2                    |      |              3           |           |                 2              | \n",
      " |    |    |    _____|_____            |   |        _________|_______                    |              |          _________|_______             |      |       _______|___        |           |              ___|_______       |  \n",
      " |    |    |   |           2           |   |       2                 2                   2              |         2                 2            |      |      |           2       |           2             2           |      | \n",
      " |    |    |   |      _____|_____      |   |    ___|____          ___|_____         _____|_____         |      ___|____          ___|_____       |      |      |        ___|___    |      _____|____      ___|___        |      |  \n",
      " 2    1    2   2     3           2     2   2   2        2        2         2       1           2        2     2        2        2         2      2      2      2       2       2   2     2          2    2       2       2      2 \n",
      " |    |    |   |     |           |     |   |   |        |        |         |       |           |        |     |        |        |         |      |      |      |       |       |   |     |          |    |       |       |      |  \n",
      " As lo-fi  as the special     effects are  ,  the     folks     who     cobbled Nemesis     together indulge the     force      of     humanity over hardware  in      a      way that George     Lucas has     long forgotten  . \n",
      "\n",
      "                            1                                        \n",
      "       _____________________|____________                             \n",
      "      |                                  0                           \n",
      "      |                          ________|_________________________   \n",
      "      |                         1                                  | \n",
      "      |              ___________|________                          |  \n",
      "      |             |                    3                         | \n",
      "      |             |            ________|_________                |  \n",
      "      |             |           3                  |               | \n",
      "      |             |        ___|___               |               |  \n",
      "      2             1       |       2              2               | \n",
      "  ____|___      ____|___    |    ___|____       ___|_______        |  \n",
      " 3        2    2        2   3   2        2     2           3       2 \n",
      " |        |    |        |   |   |        |     |           |       |  \n",
      "Like     Mike does     n't win any     points for     originality  . \n",
      "\n",
      "                                                                                    3                                                                    \n",
      "                             _______________________________________________________|__________________                                                   \n",
      "                            |                                                                          3                                                 \n",
      "                            |                                                              ____________|_______________________________________________   \n",
      "                            |                                                             3                                                            | \n",
      "                            |                                                        _____|________                                                    |  \n",
      "                            |                                                       |              3                                                   | \n",
      "                            |                                                       |          ____|___                                                |  \n",
      "                            2                                                       |         |        1                                               | \n",
      "            ________________|__________________                                     |         |     ___|_____________                                  |  \n",
      "           |                                   2                                    |         |    |                 1                                 | \n",
      "           |                 __________________|____                                |         |    |        _________|___________                      |  \n",
      "           |                |                       2                               |         |    |       |                     1                     | \n",
      "           |                |           ____________|_____                          |         |    |       |          ___________|___                  |  \n",
      "           2                |          |                  2                         2         |    |       |         |               2                 | \n",
      "    _______|______          |          |        __________|_____               _____|_____    |    |       |         |      _________|____             |  \n",
      "   |              2         |          |       |                2             1           |   |    |       2         |     |              2            | \n",
      "   |        ______|___      |          |       |           _____|____      ___|_____      |   |    |    ___|____     |     |          ____|_____       |  \n",
      "   2       2          2     3          2       2          2          2    2         1     2   4    2   2        2    2     2         2          2      2 \n",
      "   |       |          |     |          |       |          |          |    |         |     |   |    |   |        |    |     |         |          |      |  \n",
      "Stephen Earnhart      's homespun documentary Mule     Skinner     Blues has     nothing but love for its     posse  of trailer     park     denizens  . \n",
      "\n",
      "                                                                                                   2                                                                                               \n",
      "    _______________________________________________________________________________________________|________                                                                                        \n",
      "   |                                                                                                        2                                                                                      \n",
      "   |                                                                                                ________|____________________________________________________________________________________   \n",
      "   |                                                                                               3                                                                                             | \n",
      "   |                _______________________________________________________________________________|________                                                                                     |  \n",
      "   |               |                                                                                        2                                                                                    | \n",
      "   |               |                                                                            ____________|________________                                                                    |  \n",
      "   |               |                                                                           |                             2                                                                   | \n",
      "   |               |                                                                           |                 ____________|__________                                                         |  \n",
      "   |               |                                                                           |                |                       2                                                        | \n",
      "   |               |                                                                           |                |                   ____|__________                                              |  \n",
      "   |               3                                                                           |                |                  |               2                                             | \n",
      "   |      _________|________________________                                                   |                |                  |     __________|_______                                      |  \n",
      "   |     |                                  2                                                  |                |                  |    |                  1                                     | \n",
      "   |     |                    ______________|____________________________                      |                |                  |    |           _______|________________________________     |  \n",
      "   |     |                   2                                           |                     |                |                  |    |          3                                        |    | \n",
      "   |     |              _____|_________________________________          |                     |                |                  |    |     _____|_______                                 |    |  \n",
      "   |     |             3                                       |         |                     |                |                  |    |    |             1                                |    | \n",
      "   |     |     ________|_____                                  |         |                     |                |                  |    |    |      _______|___                             |    |  \n",
      "   |     |    |              1                                 |         |                     |                |                  |    |    |     |           1                            |    | \n",
      "   |     |    |     _________|______                           |         |                     |                |                  |    |    |     |    _______|____                        |    |  \n",
      "   |     |    |    |                2                          |         |                     |                |                  |    |    |     |   |            1                       |    | \n",
      "   |     |    |    |    ____________|_______                   |         |                     |                |                  |    |    |     |   |    ________|_______                |    |  \n",
      "   |     |    |    |   |                    1                  |         1                     |                3                  |    |    |     |   |   |                4               |    | \n",
      "   |     |    |    |   |      ______________|________          |      ___|___                  |         _______|_______           |    |    |     |   |   |    ____________|_______        |    |  \n",
      "   |     |    |    |   |     |                       1         |     |       1                 |        2               2          |    |    |     |   |   |   |                    3       |    | \n",
      "   |     |    |    |   |     |               ________|___      |     |    ___|________         |    ____|___         ___|____      |    |    |     |   |   |   |             _______|___    |    |  \n",
      "   2     2    3    2   2     2              1            2     2     2   2            1        2   2        2       2        2     2    2    2     2   2   1   2            4           2   2    2 \n",
      "   |     |    |    |   |     |              |            |     |     |   |            |        |   |        |       |        |     |    |    |     |   |   |   |            |           |   |    |  \n",
      "Remains  a  solid  ,   if somewhat     heavy-handed      ,  account  of the     near-disaster ... done      up      by     Howard with  a  steady  ,   if not very     imaginative      ,  hand  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for part (c).2\n",
    "# Example: display(ds.test_trees[idx])\n",
    "for i in interesting_ids[10:15]:\n",
    "    nltk.tree.Tree.pretty_print(ds.test_trees[i])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on selected examples: 73.26%\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for part (c).3\n",
    "acc = 0.0  # replace this with a real value for accuracy\n",
    "\n",
    "pred_y_interesting = nb.predict(test_x_interesting)\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "acc = accuracy_score(test_y_interesting, pred_y_interesting)\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "print(\"Accuracy on selected examples: {:.02f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
